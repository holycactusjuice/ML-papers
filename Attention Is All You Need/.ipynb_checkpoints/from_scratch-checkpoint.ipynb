{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aac33f3c-59b8-4807-88e5-c999309decb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "965b2699-2ef7-4b68-a9c8-04869a8db282",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "712f0a9b-4e0e-48db-beb7-b55f5ffde5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('shakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62853913-b7df-4748-953e-f5fc3fb72c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b985910-8c86-4ecd-b9e3-e0119aed3d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itos = {i:ch for ch,i in stoi.items()}\n",
    "\n",
    "encode = lambda s : [stoi[ch] for ch in s]\n",
    "decode = lambda l : ''.join([itos[i] for i in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a32cc2ff-d306-47fb-a99f-7d06bf8d1149",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd742ad8-cd92-429b-90e2-1fe88ac2a83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "01a04983-4846-4550-a267-0cc4220c0656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimension values\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "n_embd = 32\n",
    "n_head = 4\n",
    "d_k = 32\n",
    "d_v = d_k\n",
    "n_block = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "37387433-bd0c-4cbc-9a12-cc54961090b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    # need to get batch_size number of samples\n",
    "    # choose from the specified split\n",
    "    split_data = train_data if split == 'train' else val_data\n",
    "\n",
    "    # get batch_size number of starting indicies\n",
    "    ix = torch.randint(len(split_data) - block_size, (batch_size,))\n",
    "\n",
    "    # for each i in ix,\n",
    "    # i is the starting index\n",
    "    # so we take from i to i + block_size\n",
    "    x = torch.stack([split_data[i:i+block_size] for i in ix])\n",
    "    # y is just the element after each x\n",
    "    y = torch.stack([split_data[i+1:i+block_size+1] for i in ix])\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ebe02733-61a0-4682-a1d5-0e1d9aff4e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd, n_head, block_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_embd = n_embd\n",
    "        self.n_head = n_head\n",
    "        self.d_k = n_embd // n_head\n",
    "        \n",
    "        assert (self.n_head * self.d_k == self.n_embd), \"Embedding size not divisible by number of heads\"\n",
    "\n",
    "        self.query = nn.Linear(self.n_embd, self.d_k)\n",
    "        self.key = nn.Linear(self.n_embd, self.d_k)\n",
    "        self.value = nn.Linear(self.n_embd, self.d_k)\n",
    "\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) # (T, T)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # essentially doing X @ W_Q\n",
    "        # dims are (B, T, C) @ (T, n_embd) -> (B, T, n_embd)\n",
    "        Q = self.query(X)\n",
    "        K = self.key(X)\n",
    "        V = self.value(X)\n",
    "\n",
    "        # ----- QUESTION: Which dimensions do we transpose? -----\n",
    "        # The shape of K is (B, T, n_embd)\n",
    "        # We want to matrix multiply with Q, which is also (B, T, n_embd)\n",
    "        # We ignore the batch dimension, so we transpose T and n_embd\n",
    "        # ----- ANSWER: The last two dimensions, -2 and -1\n",
    "        weights = Q @ K.transpose(-2, -1) # we want to transpose the last two dimensions\n",
    "        weights /= self.d_k**0.5\n",
    "\n",
    "        # dims are now (B, T, T)\n",
    "\n",
    "        # Mask the weights to be lower-triangular\n",
    "        # This prevents the model from attending to future tokens\n",
    "        weights = weights.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        \n",
    "        # ----- QUESTION: Which dimension do we want to softmax over? -----\n",
    "        # The shape of out is currently (B, T, T)\n",
    "        # - B is the batch dimension\n",
    "        # - since we produced this through Q @ K^T,\n",
    "        #     - the first T represents the query tokens (since it came from Q)\n",
    "        #     - the second T represents the key tokens (since it came from K)\n",
    "        # we want the values in the key dimension to become probabilities,\n",
    "        # so we softmax over the second T\n",
    "        # ----- ANSWER: The key dimension, -1 -----\n",
    "        weights = weights.softmax(dim=-1)\n",
    "        weights = self.dropout(weights)\n",
    "        out = weights @ V\n",
    "\n",
    "        # dims are now (B, T, d_k)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b985f007-6374-4180-a08a-be0b58319a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd, n_head, block_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_embd = n_embd\n",
    "        self.n_head = n_head\n",
    "\n",
    "        self.heads = nn.ModuleList([Head(n_embd, n_head, block_size) for _ in range(n_head)])\n",
    "        self.projection = nn.Linear(n_embd, n_embd)\n",
    "\n",
    "    def forward(self, X):\n",
    "        out = torch.cat([head(X) for head in self.heads], dim=-1)\n",
    "        out = self.projection(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6fb465cc-1d58-45c1-8219-4b0da289e143",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "\n",
    "        # LayerNorm includes scale (gamma) and shift (beta) parameters\n",
    "        self.gamma = nn.Parameter(torch.ones(dim))\n",
    "        self.beta = nn.Parameter(torch.zeros(dim))\n",
    "\n",
    "    def forward(self, X):\n",
    "        # ----- QUESTION: Which dimension do we want to normalize across? -----\n",
    "        # Our dims are (B, T, n_embd)\n",
    "        # - B is the number of batches\n",
    "        # - T is the number of tokens in each sequence\n",
    "        # - n_embd is the embedding dimension, or the number of features that represents each token\n",
    "        # We want to normalize across the feature dimension to normalize the activations of each token independently.\n",
    "        # This ensures that each token has a similar distrbution of activations, regardless of its position in the sequence.\n",
    "        # ----- ANSWER: The feature dimension, -1\n",
    "        X = X - X.mean(dim=-1, keepdim=True)\n",
    "        X = X / torch.sqrt((X.var(dim=-1, keepdim=True) + 1e-5))\n",
    "        X = self.gamma * X + self.beta\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1ae02d7e-cbe0-4772-b2fa-a9a4da273015",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        hidden_dim = 4 * input_dim\n",
    "\n",
    "        self.layers = nn.Sequential(OrderedDict([\n",
    "            ('fc1', nn.Linear(input_dim, hidden_dim)),\n",
    "            ('relu', nn.ReLU()),\n",
    "            ('fc2', nn.Linear(hidden_dim, output_dim))\n",
    "        ]))\n",
    "\n",
    "    def forward(self, X):\n",
    "        out = self.layers(X)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "33b4dd25-fe95-4529-8d73-625c0e587132",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd, n_head, block_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = MultiHeadAttention(n_embd, n_head, block_size)\n",
    "        self.ln1 = LayerNorm(n_embd)\n",
    "        self.feedforward = FeedForward(n_embd, n_embd)\n",
    "        self.ln2 = LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X + self.attention(self.ln1(X))\n",
    "        X = X + self.feedforward(self.ln2(X))\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1e6a2510-524b-414d-b7d9-315736482ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, n_embd, n_head, block_size, n_block):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        # LEAVING OUT THE POSITION ENCODING FOR NOW\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head, block_size) for _ in range(n_block)]) # unpack list\n",
    "        # After the blocks, the dim is (B, T, n_embd)\n",
    "        # We want to project this to probabiliites for the next character, so we need (n_embd, vocab_size)\n",
    "        # ----- QUESTION: Why is the T dimension still here and what does it represent? -----\n",
    "        # The T dimension is the time dimension. Recall that we are passing in multiple examples with one sequence.\n",
    "        # This is because we are simultaneously predicting the 2nd char from the 1st, the 3rd from the 1st and 2nd, etc.\n",
    "        # ----- ANSWER: We have multiple time-dependent examples in one input sequence.\n",
    "        self.projection = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # (B, T)\n",
    "        X_embd = self.embedding_table(X)\n",
    "        # (B, T, n_embd)\n",
    "        X_blocks = self.blocks(X_embd)\n",
    "        # (B, T, n_embd)\n",
    "        logits = self.projection(X_blocks)\n",
    "        # (B, T, vocab_size)\n",
    "        # probs = logits.softmax(dim=-1)\n",
    "        # (B, T, vocab_size)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cee40e24-d7e0-4bad-8a26-1ecff72f603c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Transformer.forward() takes 2 positional arguments but 6 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[62]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m x, y = get_batch(\u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m t = \u001b[43mTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_embd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_head\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_block\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m result = t(x)\n\u001b[32m      4\u001b[39m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[31mTypeError\u001b[39m: Transformer.forward() takes 2 positional arguments but 6 were given"
     ]
    }
   ],
   "source": [
    "x, y = get_batch('train')\n",
    "t = Transformer(vocab_size, n_embd, n_head, block_size, n_block)\n",
    "result = t(x)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87947570-c66c-4e4f-9ddf-ee698e8cccfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a13e29-dcd7-4ba9-bec1-42baa9b22e08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
