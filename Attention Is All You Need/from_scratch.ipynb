{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aac33f3c-59b8-4807-88e5-c999309decb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "965b2699-2ef7-4b68-a9c8-04869a8db282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "712f0a9b-4e0e-48db-beb7-b55f5ffde5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('shakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62853913-b7df-4748-953e-f5fc3fb72c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b985910-8c86-4ecd-b9e3-e0119aed3d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itos = {i:ch for ch,i in stoi.items()}\n",
    "\n",
    "encode = lambda s : [stoi[ch] for ch in s]\n",
    "decode = lambda l : ''.join([itos[i] for i in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a32cc2ff-d306-47fb-a99f-7d06bf8d1149",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd742ad8-cd92-429b-90e2-1fe88ac2a83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01a04983-4846-4550-a267-0cc4220c0656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimension values\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "n_embd = 32\n",
    "n_head = 4\n",
    "d_k = 32\n",
    "d_v = d_k\n",
    "n_block = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "37387433-bd0c-4cbc-9a12-cc54961090b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    # need to get batch_size number of samples\n",
    "    # choose from the specified split\n",
    "    split_data = train_data if split == 'train' else val_data\n",
    "\n",
    "    # get batch_size number of starting indicies\n",
    "    ix = torch.randint(len(split_data) - block_size, (batch_size,))\n",
    "\n",
    "    # for each i in ix,\n",
    "    # i is the starting index\n",
    "    # so we take from i to i + block_size\n",
    "    x = torch.stack([split_data[i:i+block_size] for i in ix])\n",
    "    # y is just the element after each x\n",
    "    y = torch.stack([split_data[i+1:i+block_size+1] for i in ix])\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ebe02733-61a0-4682-a1d5-0e1d9aff4e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd, n_head, block_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_embd = n_embd\n",
    "        self.n_head = n_head\n",
    "        self.d_k = n_embd // n_head\n",
    "        \n",
    "        assert (self.n_head * self.d_k == self.n_embd), \"Embedding size not divisible by number of heads\"\n",
    "\n",
    "        self.query = nn.Linear(self.n_embd, self.d_k)\n",
    "        self.key = nn.Linear(self.n_embd, self.d_k)\n",
    "        self.value = nn.Linear(self.n_embd, self.d_k)\n",
    "\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) # (T, T)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, X):\n",
    "        B, T, C = X.shape\n",
    "        # essentially doing X @ W_Q\n",
    "        # dims are (B, T, C) @ (T, n_embd) -> (B, T, n_embd)\n",
    "        Q = self.query(X)\n",
    "        K = self.key(X)\n",
    "        V = self.value(X)\n",
    "\n",
    "        # ----- QUESTION: Which dimensions do we transpose? -----\n",
    "        # The shape of K is (B, T, n_embd)\n",
    "        # We want to matrix multiply with Q, which is also (B, T, n_embd)\n",
    "        # We ignore the batch dimension, so we transpose T and n_embd\n",
    "        # ----- ANSWER: The last two dimensions, -2 and -1\n",
    "        weights = Q @ K.transpose(-2, -1) # we want to transpose the last two dimensions\n",
    "        weights /= self.d_k**0.5\n",
    "\n",
    "        # dims are now (B, T, T)\n",
    "\n",
    "        # Mask the weights to be lower-triangular\n",
    "        # This prevents the model from attending to future tokens\n",
    "        weights = weights.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        \n",
    "        # ----- QUESTION: Which dimension do we want to softmax over? -----\n",
    "        # The shape of out is currently (B, T, T)\n",
    "        # - B is the batch dimension\n",
    "        # - since we produced this through Q @ K^T,\n",
    "        #     - the first T represents the query tokens (since it came from Q)\n",
    "        #     - the second T represents the key tokens (since it came from K)\n",
    "        # we want the values in the key dimension to become probabilities,\n",
    "        # so we softmax over the second T\n",
    "        # ----- ANSWER: The key dimension, -1 -----\n",
    "        weights = weights.softmax(dim=-1)\n",
    "        weights = self.dropout(weights)\n",
    "        out = weights @ V\n",
    "\n",
    "        # dims are now (B, T, d_k)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b985f007-6374-4180-a08a-be0b58319a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd, n_head, block_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_embd = n_embd\n",
    "        self.n_head = n_head\n",
    "\n",
    "        self.heads = nn.ModuleList([Head(n_embd, n_head, block_size) for _ in range(n_head)])\n",
    "        self.projection = nn.Linear(n_embd, n_embd)\n",
    "\n",
    "    def forward(self, X):\n",
    "        out = torch.cat([head(X) for head in self.heads], dim=-1)\n",
    "        out = self.projection(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6fb465cc-1d58-45c1-8219-4b0da289e143",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "\n",
    "        # LayerNorm includes scale (gamma) and shift (beta) parameters\n",
    "        self.gamma = nn.Parameter(torch.ones(dim))\n",
    "        self.beta = nn.Parameter(torch.zeros(dim))\n",
    "\n",
    "    def forward(self, X):\n",
    "        # ----- QUESTION: Which dimension do we want to normalize across? -----\n",
    "        # Our dims are (B, T, n_embd)\n",
    "        # - B is the number of batches\n",
    "        # - T is the number of tokens in each sequence\n",
    "        # - n_embd is the embedding dimension, or the number of features that represents each token\n",
    "        # We want to normalize across the feature dimension to normalize the activations of each token independently.\n",
    "        # This ensures that each token has a similar distrbution of activations, regardless of its position in the sequence.\n",
    "        # ----- ANSWER: The feature dimension, -1\n",
    "        X = X - X.mean(dim=-1, keepdim=True)\n",
    "        X = X / torch.sqrt((X.var(dim=-1, keepdim=True) + 1e-5))\n",
    "        X = self.gamma * X + self.beta\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1ae02d7e-cbe0-4772-b2fa-a9a4da273015",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        hidden_dim = 4 * input_dim\n",
    "\n",
    "        self.layers = nn.Sequential(OrderedDict([\n",
    "            ('fc1', nn.Linear(input_dim, hidden_dim)),\n",
    "            ('relu', nn.ReLU()),\n",
    "            ('fc2', nn.Linear(hidden_dim, output_dim))\n",
    "        ]))\n",
    "\n",
    "    def forward(self, X):\n",
    "        out = self.layers(X)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "33b4dd25-fe95-4529-8d73-625c0e587132",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd, n_head, block_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = MultiHeadAttention(n_embd, n_head, block_size)\n",
    "        self.ln1 = LayerNorm(n_embd)\n",
    "        self.feedforward = FeedForward(n_embd, n_embd)\n",
    "        self.ln2 = LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X + self.attention(self.ln1(X))\n",
    "        X = X + self.feedforward(self.ln2(X))\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1e6a2510-524b-414d-b7d9-315736482ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, n_embd, n_head, block_size, n_block):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        # LEAVING OUT THE POSITION ENCODING FOR NOW\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head, block_size) for _ in range(n_block)]) # unpack list\n",
    "        # After the blocks, the dim is (B, T, n_embd)\n",
    "        # We want to project this to probabiliites for the next character, so we need (n_embd, vocab_size)\n",
    "        # ----- QUESTION: Why is the T dimension still here and what does it represent? -----\n",
    "        # The T dimension is the time dimension. Recall that we are passing in multiple examples with one sequence.\n",
    "        # This is because we are simultaneously predicting the 2nd char from the 1st, the 3rd from the 1st and 2nd, etc.\n",
    "        # ----- ANSWER: We have multiple time-dependent examples in one input sequence.\n",
    "        self.projection = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # (B, T)\n",
    "        X_embd = self.embedding_table(X)\n",
    "        # (B, T, n_embd)\n",
    "        X_blocks = self.blocks(X_embd)\n",
    "        # (B, T, n_embd)\n",
    "        logits = self.projection(X_blocks)\n",
    "        # (B, T, vocab_size)\n",
    "        # probs = logits.softmax(dim=-1)\n",
    "        # (B, T, vocab_size)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cee40e24-d7e0-4bad-8a26-1ecff72f603c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.7082,  0.4386,  0.3150,  ..., -0.6711, -0.2957, -0.3496],\n",
       "         [-0.6630,  0.0501,  0.2692,  ...,  0.2827,  0.1718, -0.5782],\n",
       "         [-1.0980,  0.6906, -0.2351,  ..., -0.0498,  0.0634, -1.2158],\n",
       "         ...,\n",
       "         [-0.4532,  1.1237,  0.5731,  ...,  0.6057,  0.0154, -0.7607],\n",
       "         [-1.6613,  0.9826,  0.6812,  ...,  0.2089, -0.0349, -1.4035],\n",
       "         [-1.3014,  0.2951,  0.0619,  ..., -1.2210,  0.5005, -1.3925]],\n",
       "\n",
       "        [[ 0.7519,  0.8217,  0.4891,  ..., -0.3706,  0.1204, -0.1729],\n",
       "         [ 0.6805,  0.7693,  0.2407,  ..., -0.2774,  0.1304, -0.2126],\n",
       "         [ 0.0132,  0.6612,  0.6842,  ..., -0.2517, -0.2239, -0.0087],\n",
       "         ...,\n",
       "         [-1.0308, -0.1150, -0.2455,  ..., -0.3665,  0.1091, -1.4469],\n",
       "         [-1.1102, -0.1351,  0.3146,  ..., -0.5200, -1.2797,  1.1264],\n",
       "         [ 0.3526,  0.2000, -0.6483,  ...,  0.4584, -0.4952,  0.4982]],\n",
       "\n",
       "        [[ 0.6959,  0.8678,  0.4947,  ..., -0.3954,  0.1133, -0.2825],\n",
       "         [ 0.1415,  0.0622, -0.7921,  ..., -0.0832,  1.2491,  0.7850],\n",
       "         [-0.2325,  0.8581, -0.8626,  ..., -0.0502, -0.4756, -0.4102],\n",
       "         ...,\n",
       "         [-0.3353, -0.1579,  0.3735,  ..., -0.2244, -0.2342,  0.3386],\n",
       "         [ 0.0538,  1.1633,  0.4562,  ...,  0.5795, -0.1472, -0.2736],\n",
       "         [-0.6918, -0.1855, -0.7275,  ...,  0.1929, -0.0696,  0.2234]],\n",
       "\n",
       "        [[-1.3669,  0.7487,  0.6346,  ...,  0.2505,  0.5270, -0.9818],\n",
       "         [-1.3348,  0.0984, -0.3315,  ...,  0.1304, -0.5322, -0.5656],\n",
       "         [-0.8610,  0.7487, -1.0592,  ..., -0.0345, -0.3782, -0.6270],\n",
       "         ...,\n",
       "         [ 0.4117, -0.8394, -0.4766,  ...,  0.9222, -0.0566, -0.5517],\n",
       "         [-1.5373,  0.9094,  0.3657,  ...,  0.2423, -0.1493, -1.2586],\n",
       "         [-1.1360,  0.1612, -0.3646,  ...,  0.3604, -0.6585, -0.5892]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = get_batch('train')\n",
    "t = Transformer(vocab_size, n_embd, n_head, block_size, n_block)\n",
    "result = t(x)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87947570-c66c-4e4f-9ddf-ee698e8cccfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a13e29-dcd7-4ba9-bec1-42baa9b22e08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f0a1fd-c239-4c8a-9207-5288a6d83798",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
